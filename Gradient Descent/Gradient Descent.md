The gradient descent algorithm is a widely used optimization technique that is applied to a variety of machine learning problems, including linear regression, logistic regression, and neural networks. In this algorithm, a set of input values (X) and corresponding output values (Y) are used to train a model that can predict the output values for new input values. The goal is to iteratively update the model parameters in the direction of the negative gradient of the cost function, which measures the difference between the predicted and actual output values. By minimizing the cost function using the gradient descent algorithm, the model can be optimized to provide accurate predictions for new input values.

![image](https://github.com/MArya80/Machine-Learning-From-Scratch/assets/92305900/bd310d16-b90c-4c3f-b756-b834801f60b0)

The image above depicts the hypothesis function, which is used to predict the output value for a given input vector in a machine learning model. The goal of the model is to minimize the difference between the predicted output value and its corresponding true value (y). This is achieved by iteratively updating the model parameters to minimize the cost function, which measures the difference between the predicted output value and the true output value.
To this end, we define the cost function as below:

![image](https://github.com/MArya80/Machine-Learning-From-Scratch/assets/92305900/b5f55f52-5b38-42a5-a877-cc3a7baeb145)

The model parameters are updated at each iteration of the optimization algorithm using the following formula:

![image](https://github.com/MArya80/Machine-Learning-From-Scratch/assets/92305900/4427639d-0290-4ca1-b787-76d129f855bd)

